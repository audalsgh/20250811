# 3ì¼ê°„ Roboflow ì°¨ì„ ì¸ì‹ í”„ë¡œì íŠ¸ ì§„í–‰ì˜ˆì •
[Roboflow í”„ë¡œì íŠ¸ ê³„íš](https://docs.google.com/document/d/1rxQHvxAIZM0pTspVIDAUx2ZEmjZRNSW-f6OrcitXFqM/edit?tab=t.0)<br>
[í…ì„œRTì™€ íŒŒì´í† ì¹˜ ë¹„êµ ì½”ë“œëª¨ìŒ](https://docs.google.com/document/d/179l9DqTYZJ1oGDWNA-TGtFZjs1PprWVoVlV-dTw_XrM/edit?tab=t.0)

## SegFormer ëª¨ë¸ì€ ëŸ°íŒŒë“œì—ì„œ ì‹¤ìŠµí•´ë³¼ê²ƒ.
1. YOLOv11 segmentation ì‚¬ë¬¼ì¸ì‹ 19ê°œì •ë„ë¥¼ ai studioì— ë¬¼ì–´ë´ì„œ colabì—ì„œ ì‹¤í–‰í•˜ê¸°.
[ì½”ë©ì—ì„œ ì‹¤í–‰í•œ ì½”ë“œ]()
<img width="2404" height="1080" alt="image" src="https://github.com/user-attachments/assets/3ac1df3b-cefc-4ee8-93b9-8600d73b37fe" /><br>
-> ì°¨ì„ ê°™ì€ê±° ë³´ë‹¨ ì‚¬ë¬¼ì¸ì‹ì„ ëª©í‘œë¡œ í•¨.

2. í—ˆê¹…í˜ì´ìŠ¤ ì½”ë“œëŠ” ëŸ°íŒŒë“œ.
```
import torch
import cv2
import numpy as np
from PIL import Image
from transformers import pipeline
import warnings
warnings.filterwarnings('ignore')

def process_video_all_objects(input_video, output_video):
    """ëª¨ë“  ê°ì²´ë¥¼ ìƒ‰ìƒë³„ë¡œ í‘œì‹œí•˜ëŠ” ì„¸ê·¸ë©˜í…Œì´ì…˜"""

    print(f"ğŸ¬ {input_video} ì²˜ë¦¬ ì‹œì‘...")

    # 1. Hugging Face ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ
    try:
        segmenter = pipeline(
            "image-segmentation",
            model="nvidia/segformer-b0-finetuned-cityscapes-1024-1024",
            device=0 if torch.cuda.is_available() else -1
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 2. ê°ì²´ë³„ ìƒ‰ìƒ ì •ì˜ (BGR í˜•ì‹)
    colors = {
        'road': [0, 255, 0],          # ë…¹ìƒ‰
        'sidewalk': [255, 255, 0],    # ë…¸ë€ìƒ‰
        'building': [128, 128, 128],   # íšŒìƒ‰
        'wall': [128, 0, 0],          # ê°ˆìƒ‰
        'fence': [255, 0, 255],       # ë§ˆì  íƒ€
        'pole': [0, 255, 255],        # ì‹œì•ˆ
        'traffic light': [0, 0, 255], # ë¹¨ê°„ìƒ‰ - ì‹ í˜¸ë“±
        'traffic sign': [255, 255, 255], # í°ìƒ‰ - í‘œì§€íŒ
        'vegetation': [0, 128, 0],     # ì–´ë‘ìš´ ë…¹ìƒ‰
        'terrain': [128, 64, 0],      # ê°ˆìƒ‰
        'sky': [255, 128, 0],         # í•˜ëŠ˜ìƒ‰
        'person': [255, 0, 0],        # íŒŒë€ìƒ‰ - ì‚¬ëŒ
        'rider': [128, 0, 255],       # ë³´ë¼ìƒ‰
        'car': [0, 0, 128],           # ì–´ë‘ìš´ ë¹¨ê°„ìƒ‰ - ìë™ì°¨
        'truck': [255, 0, 128],       # í•‘í¬ - íŠ¸ëŸ­
        'bus': [128, 255, 0],         # ì—°ë‘ìƒ‰ - ë²„ìŠ¤
        'train': [0, 128, 255],       # ì£¼í™©ìƒ‰
        'motorcycle': [255, 128, 128], # ì—°ë¶„í™ - ì˜¤í† ë°”ì´
        'bicycle': [128, 255, 255]     # ì—°ì²­ìƒ‰ - ìì „ê±°
    }

    # 3. ë¹„ë””ì˜¤ ì—´ê¸°
    cap = cv2.VideoCapture(input_video)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"ğŸ“¹ ë¹„ë””ì˜¤: {width}x{height}, {fps}fps, {total_frames}í”„ë ˆì„")

    # 4. ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        try:
            # RGB ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)

            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
            results = segmenter(pil_image)

            # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            overlay = np.zeros_like(frame)
            detected_objects = []

            # ëª¨ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì²˜ë¦¬
            for result in results:
                label = result['label'].lower()
                mask = np.array(result['mask'])

                # í•´ë‹¹ ê°ì²´ì˜ ìƒ‰ìƒ ê°€ì ¸ì˜¤ê¸°
                if label in colors:
                    color = colors[label]
                    overlay[mask] = color
                    detected_objects.append(label)
                else:
                    # ì •ì˜ë˜ì§€ ì•Šì€ ê°ì²´ëŠ” ê¸°ë³¸ ìƒ‰ìƒ
                    overlay[mask] = [64, 64, 64]  # ì–´ë‘ìš´ íšŒìƒ‰
                    detected_objects.append(label)

            # ì›ë³¸ í”„ë ˆì„ê³¼ ì˜¤ë²„ë ˆì´ í•©ì„±
            result_frame = cv2.addWeighted(frame, 0.6, overlay, 0.4, 0)

            # í™”ë©´ì— ê²€ì¶œëœ ê°ì²´ ëª©ë¡ í‘œì‹œ
            unique_objects = list(set(detected_objects))
            y_offset = 30

            # ë°°ê²½ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (í…ìŠ¤íŠ¸ ê°€ë…ì„±)
            cv2.rectangle(result_frame, (10, 10), (400, 30 + len(unique_objects) * 25), (0, 0, 0), -1)

            for i, obj in enumerate(unique_objects):
                if obj in colors:
                    color = colors[obj]
                    # ê°ì²´ëª…ê³¼ ìƒ‰ìƒ í‘œì‹œ
                    cv2.putText(result_frame, f"{obj}", (15, y_offset + i*25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
            cv2.putText(result_frame, f"Frame: {frame_count}/{total_frames}",
                       (width-200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            out.write(result_frame)

        except Exception as e:
            print(f"í”„ë ˆì„ {frame_count} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë ˆì„ ì‚¬ìš©
            out.write(frame)

        frame_count += 1

        # ì§„í–‰ë¥  ì¶œë ¥
        if frame_count % 30 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"â³ ì§„í–‰ë¥ : {progress:.1f}% | ê²€ì¶œëœ ê°ì²´: {len(unique_objects)}ê°œ")

    cap.release()
    out.release()

    print(f"âœ… ì™„ë£Œ! ê²°ê³¼: {output_video}")
    print(f"ì´ {frame_count}í”„ë ˆì„ ì²˜ë¦¬ë¨")

# ë””ë²„ê·¸ìš©: í•œ í”„ë ˆì„ë§Œ í…ŒìŠ¤íŠ¸
def test_single_frame(video_path):
    """í•œ í”„ë ˆì„ë§Œ í…ŒìŠ¤íŠ¸í•´ì„œ ë­ê°€ ê²€ì¶œë˜ëŠ”ì§€ í™•ì¸"""

    segmenter = pipeline(
        "image-segmentation",
        model="nvidia/segformer-b0-finetuned-cityscapes-1024-1024",
        device=0 if torch.cuda.is_available() else -1
    )

    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()

    if ret:
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)

        results = segmenter(pil_image)

        print("ğŸ” ê²€ì¶œëœ ê°ì²´ë“¤:")
        for i, result in enumerate(results):
            print(f"{i+1}. {result['label']}")

        return results
    else:
        print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
        return None

# ì‹¤í–‰ ì˜µì…˜ë“¤
print("=== ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ===")
print("1. í•œ í”„ë ˆì„ í…ŒìŠ¤íŠ¸:")
print("test_single_frame('/content/2.mp4')")
print("\n2. ì „ì²´ ë¹„ë””ì˜¤ ì²˜ë¦¬:")
print("#process_video_all_objects('/content/2.mp4', '/content/2_all_objects.mp4')")

# ì‹¤í–‰
#test_single_frame('/content/1.mp4')  # ë¨¼ì € í…ŒìŠ¤íŠ¸
process_video_all_objects('/content/2.mp4', '/content/2_all_objects.mp4')  # ì „ì²´ ì²˜ë¦¬
```
