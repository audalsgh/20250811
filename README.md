# 3ì¼ê°„ Roboflow ì°¨ì„ ì¸ì‹ í”„ë¡œì íŠ¸ ì§„í–‰ 

## SegFormer ëª¨ë¸ì€ ëŸ°íŒŒë“œì—ì„œ ì‹¤ìŠµí•´ë³¼ê²ƒ.
1. YOLOv11 segmentation ì‚¬ë¬¼ì¸ì‹ 19ê°œì •ë„ë¥¼ ai studioì— ë¬¼ì–´ë´ì„œ colabì—ì„œ ì‹¤í–‰í•˜ê¸°.
```
# 3. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ëª¨ë¸ ë¡œë“œ
import torch
import cv2
import time
from ultralytics import YOLO

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì¥ì¹˜ ì„¤ì •
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ì‚¬ìš© ì¥ì¹˜: {device}")

# YOLO ë¶„í•  ëª¨ë¸ ë¡œë“œ
# --- ì—¬ê¸°ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤! ---
# 'YOLO11x-seg.pt'ë¥¼ ì‹¤ì œ íŒŒì¼ ì´ë¦„ì¸ 'yolo11x-seg.pt' (ì†Œë¬¸ì)ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
model = YOLO('yolo11x-seg.pt').to(device)

# 4. ë¹„ë””ì˜¤ ì¶”ë¡  ë° ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥

# ì…ë ¥ ë¹„ë””ì˜¤ ì—´ê¸°
video_path = "driving_video.mp4"
cap = cv2.VideoCapture(video_path)

# ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps_in = cap.get(cv2.CAP_PROP_FPS)
# ê²°ê³¼ íŒŒì¼ ì´ë¦„ì„ ë°”ê¿”ì„œ ì´ì „ ê²°ê³¼ì™€ í—·ê°ˆë¦¬ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
output_path = "driving_video_result_yolo11.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v') # ì½”ë± ì„¤ì •
out = cv2.VideoWriter(output_path, fourcc, fps_in, (width, height))

# ì§„í–‰ ìƒí™©ì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print(f"ì´ {total_frames} í”„ë ˆì„ì˜ ì˜ìƒ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì™„ë£Œ í›„ '{output_path}' íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.")

# í”„ë ˆì„ ì²˜ë¦¬ ë£¨í”„
frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    results = model.predict(frame, device=device, verbose=False)

    # ê²°ê³¼(ë¶„í•  ë§ˆìŠ¤í¬, ê²½ê³„ ìƒì ë“±)ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°
    annotated_frame = results[0].plot()

    # ì²˜ë¦¬ëœ í”„ë ˆì„ì„ ë¹„ë””ì˜¤ íŒŒì¼ì— ì“°ê¸°
    out.write(annotated_frame)
    
    frame_count += 1
    if frame_count % 100 == 0: # 100 í”„ë ˆì„ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        print(f"{frame_count} / {total_frames} í”„ë ˆì„ ì²˜ë¦¬ ì¤‘...")

# ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
print(f"ì˜ìƒ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” {output_path} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ìì› í•´ì œ
cap.release()
out.release()
cv2.destroyAllWindows()
```

2. í—ˆê¹…í˜ì´ìŠ¤ ì½”ë“œëŠ” ëŸ°íŒŒë“œ.
```
import torch
import cv2
import numpy as np
from PIL import Image
from transformers import pipeline
import warnings
warnings.filterwarnings('ignore')

def process_video_all_objects(input_video, output_video):
    """ëª¨ë“  ê°ì²´ë¥¼ ìƒ‰ìƒë³„ë¡œ í‘œì‹œí•˜ëŠ” ì„¸ê·¸ë©˜í…Œì´ì…˜"""

    print(f"ğŸ¬ {input_video} ì²˜ë¦¬ ì‹œì‘...")

    # 1. Hugging Face ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ
    try:
        segmenter = pipeline(
            "image-segmentation",
            model="nvidia/segformer-b0-finetuned-cityscapes-1024-1024",
            device=0 if torch.cuda.is_available() else -1
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 2. ê°ì²´ë³„ ìƒ‰ìƒ ì •ì˜ (BGR í˜•ì‹)
    colors = {
        'road': [0, 255, 0],          # ë…¹ìƒ‰
        'sidewalk': [255, 255, 0],    # ë…¸ë€ìƒ‰
        'building': [128, 128, 128],   # íšŒìƒ‰
        'wall': [128, 0, 0],          # ê°ˆìƒ‰
        'fence': [255, 0, 255],       # ë§ˆì  íƒ€
        'pole': [0, 255, 255],        # ì‹œì•ˆ
        'traffic light': [0, 0, 255], # ë¹¨ê°„ìƒ‰ - ì‹ í˜¸ë“±
        'traffic sign': [255, 255, 255], # í°ìƒ‰ - í‘œì§€íŒ
        'vegetation': [0, 128, 0],     # ì–´ë‘ìš´ ë…¹ìƒ‰
        'terrain': [128, 64, 0],      # ê°ˆìƒ‰
        'sky': [255, 128, 0],         # í•˜ëŠ˜ìƒ‰
        'person': [255, 0, 0],        # íŒŒë€ìƒ‰ - ì‚¬ëŒ
        'rider': [128, 0, 255],       # ë³´ë¼ìƒ‰
        'car': [0, 0, 128],           # ì–´ë‘ìš´ ë¹¨ê°„ìƒ‰ - ìë™ì°¨
        'truck': [255, 0, 128],       # í•‘í¬ - íŠ¸ëŸ­
        'bus': [128, 255, 0],         # ì—°ë‘ìƒ‰ - ë²„ìŠ¤
        'train': [0, 128, 255],       # ì£¼í™©ìƒ‰
        'motorcycle': [255, 128, 128], # ì—°ë¶„í™ - ì˜¤í† ë°”ì´
        'bicycle': [128, 255, 255]     # ì—°ì²­ìƒ‰ - ìì „ê±°
    }

    # 3. ë¹„ë””ì˜¤ ì—´ê¸°
    cap = cv2.VideoCapture(input_video)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"ğŸ“¹ ë¹„ë””ì˜¤: {width}x{height}, {fps}fps, {total_frames}í”„ë ˆì„")

    # 4. ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        try:
            # RGB ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)

            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
            results = segmenter(pil_image)

            # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            overlay = np.zeros_like(frame)
            detected_objects = []

            # ëª¨ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì²˜ë¦¬
            for result in results:
                label = result['label'].lower()
                mask = np.array(result['mask'])

                # í•´ë‹¹ ê°ì²´ì˜ ìƒ‰ìƒ ê°€ì ¸ì˜¤ê¸°
                if label in colors:
                    color = colors[label]
                    overlay[mask] = color
                    detected_objects.append(label)
                else:
                    # ì •ì˜ë˜ì§€ ì•Šì€ ê°ì²´ëŠ” ê¸°ë³¸ ìƒ‰ìƒ
                    overlay[mask] = [64, 64, 64]  # ì–´ë‘ìš´ íšŒìƒ‰
                    detected_objects.append(label)

            # ì›ë³¸ í”„ë ˆì„ê³¼ ì˜¤ë²„ë ˆì´ í•©ì„±
            result_frame = cv2.addWeighted(frame, 0.6, overlay, 0.4, 0)

            # í™”ë©´ì— ê²€ì¶œëœ ê°ì²´ ëª©ë¡ í‘œì‹œ
            unique_objects = list(set(detected_objects))
            y_offset = 30

            # ë°°ê²½ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (í…ìŠ¤íŠ¸ ê°€ë…ì„±)
            cv2.rectangle(result_frame, (10, 10), (400, 30 + len(unique_objects) * 25), (0, 0, 0), -1)

            for i, obj in enumerate(unique_objects):
                if obj in colors:
                    color = colors[obj]
                    # ê°ì²´ëª…ê³¼ ìƒ‰ìƒ í‘œì‹œ
                    cv2.putText(result_frame, f"{obj}", (15, y_offset + i*25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
            cv2.putText(result_frame, f"Frame: {frame_count}/{total_frames}",
                       (width-200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            out.write(result_frame)

        except Exception as e:
            print(f"í”„ë ˆì„ {frame_count} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë ˆì„ ì‚¬ìš©
            out.write(frame)

        frame_count += 1

        # ì§„í–‰ë¥  ì¶œë ¥
        if frame_count % 30 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"â³ ì§„í–‰ë¥ : {progress:.1f}% | ê²€ì¶œëœ ê°ì²´: {len(unique_objects)}ê°œ")

    cap.release()
    out.release()

    print(f"âœ… ì™„ë£Œ! ê²°ê³¼: {output_video}")
    print(f"ì´ {frame_count}í”„ë ˆì„ ì²˜ë¦¬ë¨")

# ë””ë²„ê·¸ìš©: í•œ í”„ë ˆì„ë§Œ í…ŒìŠ¤íŠ¸
def test_single_frame(video_path):
    """í•œ í”„ë ˆì„ë§Œ í…ŒìŠ¤íŠ¸í•´ì„œ ë­ê°€ ê²€ì¶œë˜ëŠ”ì§€ í™•ì¸"""

    segmenter = pipeline(
        "image-segmentation",
        model="nvidia/segformer-b0-finetuned-cityscapes-1024-1024",
        device=0 if torch.cuda.is_available() else -1
    )

    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()

    if ret:
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)

        results = segmenter(pil_image)

        print("ğŸ” ê²€ì¶œëœ ê°ì²´ë“¤:")
        for i, result in enumerate(results):
            print(f"{i+1}. {result['label']}")

        return results
    else:
        print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
        return None

# ì‹¤í–‰ ì˜µì…˜ë“¤
print("=== ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ===")
print("1. í•œ í”„ë ˆì„ í…ŒìŠ¤íŠ¸:")
print("test_single_frame('/content/2.mp4')")
print("\n2. ì „ì²´ ë¹„ë””ì˜¤ ì²˜ë¦¬:")
print("#process_video_all_objects('/content/2.mp4', '/content/2_all_objects.mp4')")

# ì‹¤í–‰
#test_single_frame('/content/1.mp4')  # ë¨¼ì € í…ŒìŠ¤íŠ¸
process_video_all_objects('/content/2.mp4', '/content/2_all_objects.mp4')  # ì „ì²´ ì²˜ë¦¬
```
