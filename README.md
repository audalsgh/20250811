# 3ì¼ê°„ Roboflow ì°¨ì„ ì¸ì‹ í”„ë¡œì íŠ¸ ì§„í–‰ì˜ˆì •
[Roboflow í”„ë¡œì íŠ¸ ê³„íš](https://docs.google.com/document/d/1rxQHvxAIZM0pTspVIDAUx2ZEmjZRNSW-f6OrcitXFqM/edit?tab=t.0)<br>
[í…ì„œRTì™€ íŒŒì´í† ì¹˜ ë¹„êµ ì½”ë“œëª¨ìŒ](https://docs.google.com/document/d/179l9DqTYZJ1oGDWNA-TGtFZjs1PprWVoVlV-dTw_XrM/edit?tab=t.0)<br>
ì¶”í›„ì— ì‚¬ìš©í•  ai studio ê°€ì…í•˜ê¸°

## 1. YOLOv11 segmentation ì‚¬ë¬¼ì¸ì‹ ì½”ë“œ colabì—ì„œ ì‹¤ìŠµ
- YOLOv11 segmentationì€ 19ê°œ ì •ë„ì˜ ì‚¬ë¬¼ì¸ì‹ì´ ëœë‹¤ê³  ì•Œë ¤ì ¸ìˆìŒ.<br>
[ì½”ë©ì—ì„œ ì‹¤í–‰í•œ ì½”ë“œ](0811_YOLOv11_Segmentation.ipynb)
<img width="2404" height="1080" alt="image" src="https://github.com/user-attachments/assets/3ac1df3b-cefc-4ee8-93b9-8600d73b37fe" /><br>
-> ì°¨ì„ ê°™ì€ê±° ë³´ë‹¨ ì‚¬ë¬¼ì¸ì‹ì„ ëª©í‘œë¡œ í•¨.

## 2. SegFormer ëª¨ë¸ì´ë€?
SegFormer ëª¨ë¸ì€ ëŸ°íŒŒë“œì—ì„œ ì‹¤ìŠµí•´ë³¼ê²ƒ.
- NVIDIAì—ì„œ ì œì•ˆí•œ Transformer ê¸°ë°˜ì˜ ì„¸ê·¸ë©˜í…Œì´ì…˜(Segmentation) ëª¨ë¸
- ê¸°ì¡´ëª¨ë¸ì€ CNN(Convolutional Neural Network)ì„ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í–ˆì§€ë§Œ, SegFormerëŠ” Efficient Self-Attention ê¸°ë°˜ì˜ Transformer Encoderë¥¼ ì‚¬ìš©
- Hierarchical Transformer Encoder : ì—¬ëŸ¬ í•´ìƒë„ì˜ í”¼ì²˜ë§µì„ ìƒì„±í•´ ë‹¤ë‹¨ê³„(Context + Detail) ì •ë³´ë¥¼ í†µí•©.
- ë‹¤ì–‘í•œ ì…ë ¥ í¬ê¸°ì™€ í•´ìƒë„ì—ì„œ ê°•í•œ ì„±ëŠ¥
  - Cityscapes, ADE20K ê°™ì€ ê³ í•´ìƒë„ ë°ì´í„°ì…‹ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„.
  - ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ë„ ì ìš© ê°€ëŠ¥í•  ì •ë„ë¡œ ê²½ëŸ‰í™” ê°€ëŠ¥.
- Lightweight MLP Decoder : ë³µì¡í•œ ë””ì½”ë” ëŒ€ì‹  ë‹¨ìˆœí•œ MLP(Multi-Layer Perceptron) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ë„ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ìŒ.
  - MLP êµ¬ì¡°ë€, ì¸ì ‘í•œ ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ì´ ì„œë¡œ ì—°ê²°ë¨ (Fully Connected Layer) ì™„ì „ ì—°ê²°ì¸µì´ë©°,<br>ëª¨ë“  ì…ë ¥ì„ ì—¬ëŸ¬ ì¸µ(Layer)ì˜ ë‰´ëŸ°(Neuron)ì„ ê±°ì¹˜ê²Œ í•˜ì—¬ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë³€í™˜í•˜ë©´ì„œ ë³µì¡í•œ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ëŠ” êµ¬ì¡°.
  - CNNì²˜ëŸ¼ ì§€ì—­ íŒ¨í„´(ì´ë¯¸ì§€ í•„í„°ë§)ì€ ëª»í•˜ì§€ë§Œ, ì „ì—­ì ì¸ ê´€ê³„ í•™ìŠµì— ê°•í•¨

```
[ì´ë¯¸ì§€ ì…ë ¥]
      â†“
[Hierarchical Transformer Encoder]
      â†“
[Lightweight MLP Decoder]
      â†“
[í”½ì…€ ë‹¨ìœ„ í´ë˜ìŠ¤ ë§µ ì¶œë ¥]
```

ì¥ì 
- CNN ì—†ì´ë„ SOTA(SOTA=State-of-the-Art) ì„±ëŠ¥ ë‹¬ì„±
- í•™ìŠµ ì†ë„ ë¹ ë¥´ê³  ì¶”ë¡  íš¨ìœ¨ì´ ë†’ìŒ
- ì‘ì€ ëª¨ë¸(SegFormer-B0)ë¶€í„° ëŒ€í˜• ëª¨ë¸(SegFormer-B5)ê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸° ì œê³µ

í™œìš© ë¶„ì•¼ ì˜ˆì‹œ
- ììœ¨ì£¼í–‰ ì°¨ëŸ‰ì˜ ë„ë¡œ ê°ì²´ ì¸ì‹
- ìœ„ì„±/í•­ê³µ ì´ë¯¸ì§€ ë¶„ì„
- ì˜ë£Œ ì˜ìƒ(CT, MRI) ì¥ê¸° ë¶„í• 
- ë¡œë´‡ ë¹„ì „

### í—ˆê¹…í˜ì´ìŠ¤(Hugging Face)ë€?
- **AI ëª¨ë¸ ê¹ƒí—ˆë¸Œë¼ê³  í• ìˆ˜ìˆë‹¤.**
- ì˜¤í”ˆì†ŒìŠ¤ AI í”Œë«í¼ì´ì ë¨¸ì‹ ëŸ¬ë‹ ì»¤ë®¤ë‹ˆí‹°ë¡œ, íŠ¹íˆ **ìì—°ì–´ ì²˜ë¦¬(NLP)**ë¥¼ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ ì‰½ê²Œ ì‚¬ìš©,ê³µìœ í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì „ ì„¸ê³„ ê°œë°œìì™€ ì—°êµ¬ìë“¤ì´ ë§Œë“  ì‚¬ì „ í•™ìŠµ(Pre-trained) ëª¨ë¸ ìˆ˜ì‹­ë§Œê°œë¥¼ ë‹¤ìš´ ê°€ëŠ¥.
- Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ : Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ëŒ€í‘œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬.<br>í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„± ëª¨ë¸ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì™€ì„œ ì¶”ë¡ ,í•™ìŠµ ê°€ëŠ¥ (PyTorch, TensorFlow, JAX ì§€ì›)
- Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ : ëŒ€ê·œëª¨ ê³µê°œ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œÂ·ì „ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” íˆ´.<br>NLPë¿ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ìŒì„± ë°ì´í„°ì…‹ë„ í¬í•¨
- Trainer API : ì´ˆë³´ìë„ ëª‡ ì¤„ ì½”ë“œë¡œ ëª¨ë¸ fine-tuning ê°€ëŠ¥í•˜ê²Œ ì§€ì›í•˜ëŠ” í•™ìŠµ ì¸í„°í˜ì´ìŠ¤

ì¥ì 
- ì†ì‰¬ìš´ ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©: ëª¨ë¸ ë¡œë“œëŠ” í•œ ì¤„ì´ë©´ ì¶©ë¶„. `from transformers import pipeline`
- ë©€í‹°í”„ë ˆì„ì›Œí¬ ì§€ì›: PyTorch, TensorFlow, JAX
- NLP, CV(ì»´í“¨í„° ë¹„ì „), ASR(ìŒì„± ì¸ì‹) ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì„ ê°€ì§.

```
import torch
import cv2
import numpy as np
from PIL import Image
from transformers import pipeline
import warnings
warnings.filterwarnings('ignore')

def process_video_all_objects(input_video, output_video):
    """ëª¨ë“  ê°ì²´ë¥¼ ìƒ‰ìƒë³„ë¡œ í‘œì‹œí•˜ëŠ” ì„¸ê·¸ë©˜í…Œì´ì…˜"""

    print(f"ğŸ¬ {input_video} ì²˜ë¦¬ ì‹œì‘...")

    # 1. Hugging Face ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ
    try:
        segmenter = pipeline(
            "image-segmentation",
            model="nvidia/segformer-b0-finetuned-cityscapes-1024-1024",
            device=0 if torch.cuda.is_available() else -1
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 2. ê°ì²´ë³„ ìƒ‰ìƒ ì •ì˜ (BGR í˜•ì‹)
    colors = {
        'road': [0, 255, 0],          # ë…¹ìƒ‰
        'sidewalk': [255, 255, 0],    # ë…¸ë€ìƒ‰
        'building': [128, 128, 128],   # íšŒìƒ‰
        'wall': [128, 0, 0],          # ê°ˆìƒ‰
        'fence': [255, 0, 255],       # ë§ˆì  íƒ€
        'pole': [0, 255, 255],        # ì‹œì•ˆ
        'traffic light': [0, 0, 255], # ë¹¨ê°„ìƒ‰ - ì‹ í˜¸ë“±
        'traffic sign': [255, 255, 255], # í°ìƒ‰ - í‘œì§€íŒ
        'vegetation': [0, 128, 0],     # ì–´ë‘ìš´ ë…¹ìƒ‰
        'terrain': [128, 64, 0],      # ê°ˆìƒ‰
        'sky': [255, 128, 0],         # í•˜ëŠ˜ìƒ‰
        'person': [255, 0, 0],        # íŒŒë€ìƒ‰ - ì‚¬ëŒ
        'rider': [128, 0, 255],       # ë³´ë¼ìƒ‰
        'car': [0, 0, 128],           # ì–´ë‘ìš´ ë¹¨ê°„ìƒ‰ - ìë™ì°¨
        'truck': [255, 0, 128],       # í•‘í¬ - íŠ¸ëŸ­
        'bus': [128, 255, 0],         # ì—°ë‘ìƒ‰ - ë²„ìŠ¤
        'train': [0, 128, 255],       # ì£¼í™©ìƒ‰
        'motorcycle': [255, 128, 128], # ì—°ë¶„í™ - ì˜¤í† ë°”ì´
        'bicycle': [128, 255, 255]     # ì—°ì²­ìƒ‰ - ìì „ê±°
    }

    # 3. ë¹„ë””ì˜¤ ì—´ê¸°
    cap = cv2.VideoCapture(input_video)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"ğŸ“¹ ë¹„ë””ì˜¤: {width}x{height}, {fps}fps, {total_frames}í”„ë ˆì„")

    # 4. ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        try:
            # RGB ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)

            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
            results = segmenter(pil_image)

            # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            overlay = np.zeros_like(frame)
            detected_objects = []

            # ëª¨ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì²˜ë¦¬
            for result in results:
                label = result['label'].lower()
                mask = np.array(result['mask'])

                # í•´ë‹¹ ê°ì²´ì˜ ìƒ‰ìƒ ê°€ì ¸ì˜¤ê¸°
                if label in colors:
                    color = colors[label]
                    overlay[mask] = color
                    detected_objects.append(label)
                else:
                    # ì •ì˜ë˜ì§€ ì•Šì€ ê°ì²´ëŠ” ê¸°ë³¸ ìƒ‰ìƒ
                    overlay[mask] = [64, 64, 64]  # ì–´ë‘ìš´ íšŒìƒ‰
                    detected_objects.append(label)

            # ì›ë³¸ í”„ë ˆì„ê³¼ ì˜¤ë²„ë ˆì´ í•©ì„±
            result_frame = cv2.addWeighted(frame, 0.6, overlay, 0.4, 0)

            # í™”ë©´ì— ê²€ì¶œëœ ê°ì²´ ëª©ë¡ í‘œì‹œ
            unique_objects = list(set(detected_objects))
            y_offset = 30

            # ë°°ê²½ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (í…ìŠ¤íŠ¸ ê°€ë…ì„±)
            cv2.rectangle(result_frame, (10, 10), (400, 30 + len(unique_objects) * 25), (0, 0, 0), -1)

            for i, obj in enumerate(unique_objects):
                if obj in colors:
                    color = colors[obj]
                    # ê°ì²´ëª…ê³¼ ìƒ‰ìƒ í‘œì‹œ
                    cv2.putText(result_frame, f"{obj}", (15, y_offset + i*25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
            cv2.putText(result_frame, f"Frame: {frame_count}/{total_frames}",
                       (width-200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            out.write(result_frame)

        except Exception as e:
            print(f"í”„ë ˆì„ {frame_count} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë ˆì„ ì‚¬ìš©
            out.write(frame)

        frame_count += 1

        # ì§„í–‰ë¥  ì¶œë ¥
        if frame_count % 30 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"â³ ì§„í–‰ë¥ : {progress:.1f}% | ê²€ì¶œëœ ê°ì²´: {len(unique_objects)}ê°œ")

    cap.release()
    out.release()

    print(f"âœ… ì™„ë£Œ! ê²°ê³¼: {output_video}")
    print(f"ì´ {frame_count}í”„ë ˆì„ ì²˜ë¦¬ë¨")

# ë””ë²„ê·¸ìš©: í•œ í”„ë ˆì„ë§Œ í…ŒìŠ¤íŠ¸
def test_single_frame(video_path):
    """í•œ í”„ë ˆì„ë§Œ í…ŒìŠ¤íŠ¸í•´ì„œ ë­ê°€ ê²€ì¶œë˜ëŠ”ì§€ í™•ì¸"""

    segmenter = pipeline(
        "image-segmentation",
        model="nvidia/segformer-b0-finetuned-cityscapes-1024-1024",
        device=0 if torch.cuda.is_available() else -1
    )

    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()

    if ret:
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)

        results = segmenter(pil_image)

        print("ğŸ” ê²€ì¶œëœ ê°ì²´ë“¤:")
        for i, result in enumerate(results):
            print(f"{i+1}. {result['label']}")

        return results
    else:
        print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
        return None

# ì‹¤í–‰ ì˜µì…˜ë“¤
print("=== ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ===")
print("1. í•œ í”„ë ˆì„ í…ŒìŠ¤íŠ¸:")
print("test_single_frame('/content/2.mp4')")
print("\n2. ì „ì²´ ë¹„ë””ì˜¤ ì²˜ë¦¬:")
print("#process_video_all_objects('/content/2.mp4', '/content/2_all_objects.mp4')")

# ì‹¤í–‰
#test_single_frame('/content/1.mp4')  # ë¨¼ì € í…ŒìŠ¤íŠ¸
process_video_all_objects('/content/2.mp4', '/content/2_all_objects.mp4')  # ì „ì²´ ì²˜ë¦¬
```
